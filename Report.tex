% VDE Template for EUSAR Papers
% Provided by Barbara Lang und Siegmar Lampe
% University of Bremen, January 2002
% English version by Jens Fischer
% German Aerospace Center (DLR), December 2005
% Additional modifications by Matthias Wei{\ss}
% FGAN, January 2009

%-----------------------------------------------------------------------------
% Type of publication
\documentclass[a4paper,10pt]{article}
%-----------------------------------------------------------------------------
% Other packets: Most packets may be downloaded from www.dante.de and
% "tcilatex.tex" can be found at (December 2005):
% http://www.mackichan.com/techtalk/v30/UsingFloat.htm
% Not all packets are necessarily needed:
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
%\usepackage{ngerman} % in german language if required
\usepackage[nooneline,bf]{caption} % Figure descriptions from left margin
\usepackage{times}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epsfig}
\input{tcilatex}
%-----------------------------------------------------------------------------
% Page Setup
\textheight24cm \textwidth17cm \columnsep6mm
\oddsidemargin-5mm                 % depending on print drivers!
\evensidemargin-5mm                % required margin size: 2cm
\headheight0cm \headsep0cm \topmargin0cm \parindent0cm
\pagestyle{empty}                  % delete footer and header
%----------------------------------------------------------------------------
% Environment definitions
\newenvironment*{mytitle}{\begin{LARGE}\bf}{\end{LARGE}\\}%
\newenvironment*{mysubtitle}{\bf}{\\[1.5ex]}%
\newenvironment*{myabstract}{\begin{Large}\bf}{\end{Large}\\[2.5ex]}%
%-----------------------------------------------------------------------------
% Using Pictures and tables:
% - Instead "table" write "tablehere" without parameters
% - Instead "figure" write "figurehere " without parameters
% - Please insert a blank line before and after \begin{figuerhere} ... \end{figurehere}
%
% CAUTION:   The first reference to a figure/table in the text should be formatted fat.
%
\makeatletter
\newenvironment{tablehere}{\def\@captype{table}}{}
\newenvironment{figurehere}{\def\@captype{figure}\vspace{2ex}}{\vspace{2ex}}
\makeatother



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% Please use capital letters in the beginning of important words as for example
\begin{mytitle}A survey on resource management policies for NUMA architectures\end{mytitle}
%
% Please do not insert a line here
%
\\
Ferrini Francesco\\
Matr. 755508, (goffredoam84@gmail.com)\\
\hspace{10ex}
Gallo Francesco\\
Matr. 755748, (loshura@hotmail.it)\\
\begin{flushright}
\emph{Report for the master course of Embedded Systems}\\
\emph{Reviser: PhD. Patrick Bellasi (bellasi@elet.polimi.it)}
\end{flushright}

Received: April, 01 2011\\
\hspace{10ex}

\begin{myabstract} Abstract \end{myabstract}
abstract

\vspace{4ex}	% Please do not remove or reduce this space here.
\begin{multicols}{2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Numa (Non-Uniform Access Memory) refers to a computer memory design choice used in multiprocessing, where the memory access time depends on the memory location relative to a processor; in fact NUMA means that it will take longer to access some regions of memory than others.

\begin{figurehere}
 \centering
 \includegraphics[width=8cm, height=4cm]{./eps/numa.jpg}
 \caption{a simple NUMA architecture design}
 \label{fig:numa}
\end{figurehere}

In Figure \ref{fig:numa} it's possible to see that in a NUMA multiprocessor the physical memory of the system is distributed among individual processors but is still globally addressable by all processors. Sometimes, it is preferable to have more than one processor in the same area. Each of this computational area containing memory block and CPU, whether it has one or more processors, is called node.Thus the cost of a memory access is dependent on where the memory unit addressed is located; relative to the processor some accesses may be local some may be remote. So, with the introduction of NUMA, data locality becomes an important factor because memory accesses have different costs depending on which memory unit is addressed. The scheduling module in the OS must cooperate with the memory management in order to let an application access data mostly from the local memory. NUMA systems can be classified in two categories, both for memory and for processor; regarding memory there are symmetrical and asymmetrical distribution, meaning that accessing different memory nodes can or cannot cost the same. Concerning processor it is possible to have homogeneous system, meaning that all processors are equal in speed and processing power, opposed to heterogeneous ones, where processors have different specifications even inside the same node. This type of architecture turns out to be exposed to some problems, and the most important are: cache coherence, scheduling and memory management.\par
\parindent 10mm In multiprocessor systems it is important to ensure cache coherency. If each processor has a cache that reflects the state of various parts of memory, it is possible that two or more caches may have copies of the same line. It is also possible that a given line may contain more than one lockable data item. If two threads make appropriately serialized changes to those data items, the result could be that both caches end up with different, incorrect versions of the line of memory. In other words, the system's state is no longer coherent because the system contains two different versions of what is supposed to be the content of a specific area of memory. There are 2 types of NUMA architectures that we have to analyse: NCC-NUMA (Non Cache Coherent) and CC-NUMA (Cache Coherent). In NCC-NUMA systems we have some software solutions, while in CC-NUMA we have a cooperation of both software and hardware solutions.\par 
\parindent 10mm Scheduling, memory management and thread migration are strictly connected.\\
In a multiprogrammed multiprocessor, the scheduler is not only responsible for deciding when to activate an application and when to suspend it, but is also responsible for determining how many processors to allocate to each application. In a scalable NUMA multiprocessor, it must further resolve the problem of which processors to allocate to which application since the memory reference times are not the same for all processor-memory pairs.\\
In NUMA systems, accesses to remote memory via interconnection are subject to various overheads. The bandwidth is lower than the bandwidth provided by the on-chip memory controller. The latency is higher as well: memory operations are processed by the local interface to the interconnect (arbitration may be needed if multiple cores access remote memory), a request is transmitted to another processor, and additional steps may be needed on this remote processor before the memory access can be done. Consequently, remote memory accesses suffer penalties of 1.5 to 2 times relative to local accesses. Good data locality is therefore highly desirable, i.e. the computations should take place on the processor that keeps their data. So memory management on these systems cannot be done without paying attention to process mapping, and a process scheduler that determines which processor executes a thread must consider where memory has been allocated.\\
With the introduction of NUMA systems, the scheduler now must also determine which processor(s) to allocate to which application. The relative positions of processors allocated to an application will affect the performance of it due to the difference in memory access times. Performance of an application may improve if all its allocated processors use only local memories \cite{Majo_memorysystem}.\\
Even among cache-coherent systems, these distributed architectures have non-constant physical distance between hardware components, causing their communication time to vary. Indeed, a core accesses local memory faster than other memory nodes. Most modern operating systems actually rely on a lazy allocation: when applications allocate virtual memory, the underlying physical pages are actually allocated upon the first access. While the primary advantage of this strategy is to decrease resource consumption, it brings an interesting feature usually referred to as first-touch: each page is allocated in the context of the thread that actually uses it first. The operating system is thus able to allocate physical pages on the memory node near the core that accesses it. However, if the first thread touching a page is not the one that will access it intensively in the future, the page may not be allocated "in the right place". For this reason, some applications manually touch pages during the initialization phase to ensure that they are allocated close to the threads that will actually access them.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Solutions and Methods}

In this chapter will be presented some solutions about the problems introduced in previous section, divided by argument. Evaluations will be considered in the next chapter.

%-----------------------------------------------------------------------------
\subsection{Cache Coherence \\ Section}

As seen before, there are 2 types of NUMA architectures that will be analysed: NCC-NUMA (Non Cache Coherent - NUMA) and CC-NUMA (Cache Coherent - NUMA). \\
\subsubsection{Cache coherence in NCC-NUMA}
In NCC-NUMA systems, \emph{Kontothanassis and Scott} \cite{Kontothanassis94softwarecache} shown a new adaptive algorithm for software cache coherence that reduces interprocessor communication and scales to large numbers of processors; they then evaluate the tradeoffs among various write policies and the effect on performance of using remote memory access. Finally they also observe that certain simple program changes can greatly improve performance. this is done by following three steps. First, they maintain directory information for the coherence protocol in uncached shared locations, and access it with ordinary loads and stores, avoiding the need to interrupt remote processors in almost all circumstances. Second, while using virtual memory to maintain coherence at the granularity of pages, they never copy pages. Instead, they map them remotely and allow the hardware to fetch cache lines on demand. Third, while allowing multiple writers for concurrency, they avoid the need to keep old copies and compute diffs by using ordinary hardware write-through or write-back to the unique main-memory copy of each page. \par 
\parindent 10mm Concerning the same problem, \emph{Petersen and Li} \cite{Li95multiprocessorcache} presents a virtual memory based cache coherence that dinamically detects and resolves potential cache inconsistency using virtual memory techniques. VM-based cache coherence trades off design simplicity against incresed software overheads; this relies on that the existing memory transaction hardware on each processor that will be use to detect share accesses that could lead to memory incoherencies and special handlers execute the appropriate action to maintain cache coherence.
When a shared address space is created, the system initializes a page table for each processor. in this tables, the entries for a shared page will always have the same virtual-to-physical address tranlation. VM software will maintain the page tables at runtime according to a chosen memory consistency model. In the design of the VM-based algorithms synchronization variables are non cacheable to avoid the page fault overhead due to locks with high contention. Shared pages will therefore either contain ordinary data, and be cacheable, or synchronization variables, and be non-cacheable. Since most memory management units (MMUs) allow data to be cacheable on a per page basis, this property is used to mark pages that contain synchronization variables as non-cacheable.
By using off-the-shelf parts, state of the art components can be used in building multi-processors without additional design and fabrication efforts to incorporate cache coherence hardware.\\
Of this algorithm there are two implementation: VM-based Sequential Consistency (SC) and VM-based Release Consistency (RC).

\subsubsection{Cache coherence in CC-NUMA}
Regarding CC-Numa, \emph{Silvia Lametti} \cite{lametti} explains there are two protocols that are used to guarantee coherence: snooping and directory-based. Snooping system use a totally ordered network to directly broadcast coherence transactions to all processors and memory. Another solution is directory-based protocol; this one transmits coherence transactions over an arbitrary point-to-point network to the corresponding home directories which, in turn, redirect them to the processors caching the line. 

\begin{figurehere}
 \centering
 \includegraphics[width=8cm, height=4cm]{./eps/SnoopeDir.png}
 \caption{Snooping and Directory-based protocol}
 \label{fig:s&d}
\end{figurehere}

In all the systems which use the automatic techniques (snooping and directory), a proper protocol must exists in order to perform the required actions atomically. Two main techniques are used: \emph{invalidation}, \emph{update}.\\
Invalidation assumes that the only valid copy of a block is one of those, usually the last one, that has changed, invalidating all other copies (e.g. MSI and MESI in Figure \ref{fig:msimesi}); while update presumes that each modifcation of a cache block copy is communicated (multicasted) to all other caches (e.g. Dragon).\par
\parindent 10mm In the next chapter, invalidation and update techniques will be analysed only on directory-based protocol because it is clear from bibliography that large NUMA system does not support well BUS architecture due to large number of cores and, thus, messages that will limit the bandwith.

\begin{figurehere}
 \centering
 \includegraphics[width=8cm, height=4cm]{./eps/msimesi.png}
 \caption{MESI and MSI protocol}
 \label{fig:msimesi}
\end{figurehere}

%-----------------------------------------------------------------------------
\subsection{Scheduling and memory management \\ Section}
\emph{Chee-Shong Wu} \cite{Wu93processorscheduling} presents two tipe of policies: \emph{static} and \emph{dynamic}. \\
In the static policy, an application is allocated a fixed number of processors when it is activated, and it keeps these processors throughout its lifetime. Reallocations are performed only when some application completes. If there are no processors available, new arrivals are not started until some active application terminates and releases its set of processors. Applications hold the same number of processors from the beginning until the end of their execution without any readjustments.\\
In the dynamic policy, the number of processors allocated to an application may vary during its execution. The scheduler in this case has the responsibility of adjusting the number of processors allocated to an application according to its time varying parallelism, as well as to the system load. The hope is to avoid processor idling by quickly switching a processor from an application that does not require it to an application that can immediately utilize it.\\
\emph{Wu} presents a new static policy, called \emph{Immediate Start Static} (ISS). He implemented this policy because allocating an extra processor to an application that has sublinear speedup will cause some delay to all subsequent applications that use the same processors even though it may reduce the execution time of the application with the extra processor. Thus, it is important for a scheduler to be sensitive to the number of applications ready for execution and the overall system load. As the queue of ready applications becomes longer or as the system load increases, fewer processors should be allocated to each application in order to minimize overall average response time. A simple approach to making the scheduler sensitive to the length of the ready application queue and to the system load is to start a new application as soon as possible ideally immediately if there are active applications with more than one processor. This will likely require preempting processors from active applications at application arrivals. In the Immediate Start Static policy, when a new application arrives, if there are idle processors they are allocated to the new application; otherwise, the application is placed in a SSDF waiting queue. An active application that uses its last processor least effectively (the application with the smallest marginal loss of losing a processor) is identified and asked to give up a processor.
When a processor is given up by an active application due to the system scheduler's request, the processor is allocated to the first application in the waiting queue. At application completions, the actions are the same as in the static policy.\par
\parindent 10mm In \emph{Majo and Gross}'s paper \cite{Majo_memorymanagement}, is shown an hybrid solution between static and dynamic policies. They focus on the NUMA-Multicore-Aware Scheduling Scheme (N-MASS) that is an extension of the standart Linux scheduler. The reason why they need a new policy is that the performance of multiprogrammed workloads varies largely with the default Linux scheduler, and simple factors like the order in which workloads are started influence the performance readings. Because Linux scheduler balance only the CPU load and do not account for data locality or cache contention, processes might be mapped so that they use the memory system in the most ineffcient way possible. N-MASS, on the other hand, is a scheduling algorithm that takes into account memory system optimizations to quickly estimate the memory behavior of the scheduled programs on runtime (e.g NUMA penalty, that is the slow down of the same program executed remotely instead of locally). The N-MASS algorithm follows three steps. First, for each processor, it sorts the list of processes homed on the processor in descending order of the processes' NUMA penalty. Second, it maps the processes onto the system using the maximum-local policy. If the pressure on the memory system of the two-processor system is unbalanced, then in the third step the algorithm refines the mapping decision produced by the maximum-local mapping (as shown in Figure \ref{fig:n-mass_algo}).
This policy currently works on a NUMA with two processors but it can be extended to more cores.

\begin{figurehere}
 \centering
 \includegraphics[width=8cm, height=4cm]{./eps/n-mass_algo.png}
 \caption{N-MASS: maps n processes onto a 2-processor NUMA-multicore system}
 \label{fig:n-mass_algo}
\end{figurehere}

The \emph{Li et al} 's paper \cite{Li07efficientoperating} presents AMPS, a scheduler that efficiently supports both SMP and NUMA-style performance-asymmetric architectures. AMPS contains three components: asymmetry-aware load balancing, fastercore-first scheduling, and NUMA-aware migration.\\
Asymmetry-aware load balancing ensures that the load on each core is proportional to its computing power. By defining a core's scaled computing power, P, to be its computing power divided by the system's minimum core computing power. At boot time, AMPS sets the scaled computing power of the core with the lowest frequency to one and any core with a F times higher frequency to F x S, where S is a less-than-one scaling factor. For any core with scaled computing power P, they define its scaled load, L, to be its run queue length divided by P. Let Lmax and Lmin be the maximum and minimum scaled load of a core in an asymmetric system. We define that the system is load-balanced if Lmax - Lmin is minor or equal then 1. In this asymmetric model, one unit of time on a core with P = x is equivalent to x units of time on a core with P = 1. Thus, by balancing threads proportionately to core computing power, AMPS facilitates fairness.\\
Faster-core-first scheduling (Figure \ref{fig:fcf}) enables threads to run on more powerful cores whenever they are under-utilized (i.e. L < 1), which load balancing alone cannot achieve in an asymmetric system. In AMPS threads move to and run on a faster core as long as the core is under-utilized. With this approach, a fast core of scaled computing power P behaves like P slow cores; in effect, they have transformed an asymmetric system into a symmetric one, which greatly simplifies the fairness support. For a newly created thread, they compute the new scaled load for each core assuming that the thread would run on it and choose the core with the minimum new scaled load to start the thread; if tied, they choose the faster core. AMPS does not allow a thread to migrate if the scaled load on the destination core after the migration can become greater than the  scaled load on the original core before the migration. This condition prevents a thread from migrating if its performance is not likely to improve.

\begin{figurehere}
 \centering
 \includegraphics[width=8cm, height=9cm]{./eps/fcf.PNG}
 \caption{Example for Faster-core-first scheduling. \emph{P} denotes scaled computing power and \emph{L} denotes scaled load}
 \label{fig:fcf}
\end{figurehere}

NUMA-aware migration dynamically predicts thread migration overheads using memory resident sets and controls migration on NUMA-style architectures.
Qualitatively, a thread's migration overhead is high if the following conditions are both true, the thread incurs a high number of last-level cache misses after the migration and a large fraction of these Last Level Cache (LLC) misses require remote memory access.
When a thread T migrates from core A to core B, the algorithm predicts the migration overhead to be high if all of the following conditions are true:

\begin{itemize}
    \item Core A and core B are in different nodes.
    \item Core A is in a node for which thread T has the maximum Resident Set Size (the resident set of a thread on node N to be the set of pages that belong to the resident set of the thread and physically reside on node N - RSS)
   \item The RSS counter value of thread T for core A's node is greater than the LLC size of core B.
to the page.
\end{itemize}

If these conditions are true and thread T migrates, it will likely incur a large number of LLC misses and remote accesses to memory in core A's node.
Whenever there is a running thread to migrate, AMPS decides whether to allow the migration using one of the following policies:

\begin{itemize}
    \item \textbf{The Always policy:} with this policy, AMPS always allows a running thread to migrate regardless of the potential overhead.
    \item \textbf{The Same-Node policy:} with this policy, AMPS allows a running thread to migrate only within the same node and It is conservative in that it prevents a thread from migrating to a remote, faster core even when the actual migration overhead might be small.
   \item \textbf{The RSS policy:} this policy uses the resident-set-based prediction algorithm. It disallows a running thread to migrate if either it predicts the migration overhead to be high, or the thread is in the memory allocation phase.
to the page.
\end{itemize}

Recent research \cite{Balakrishnan05theimpact} advocates performance-asymmetric (or heterogeneous) architectures, where a processor contains multiple cores with the same instruction set but different performance characteristics (e.g., clock speed, issue width, in-order vs. out-of-order).\par
\parindent 10mm Finally, regarding thread migration, \emph{Hofmeyr and Colmenares} \cite{juggle} introduce a practical decentralized, user-space implementation of a proactive load balancer that emphasizes portability and usability that they called Juggle. They also analyze when it is conveninet to migrate a thread or not. Juggle executes periodically, every λ milliseconds (the load-balancing period) and attempts to balance an application that has \emph{n} threads running on \emph{m} processors, where \emph{n} is greater or equal then \emph{m}. The objective is to assign threads to processors such that the threads that have made the least progress now run on the processors with the lightest loads (the "fastest" processors). In practice, we classify threads as either ahead (above-average progress) or behind (below-average progress), and we classify processors as either fast or slow, according to whether they are less or more heavily loaded than average, respectively.  Juggle attempts to assign ahead threads toslow processors, and behind threads to fast processors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}

In this section it will be discussed the difference between the proposed solutions above giving some value coming from direct confrontation.

%-----------------------------------------------------------------------------
\subsection{Cache coherence \\ Section}

\subsubsection{Cache coherence in NCC-NUMA}
Hardware coherence mechanisms for large-scalemachines are complex and costly, but existing software mechanisms for message-passing machines have not provided a performance-competitive solution. \par 
\parindent 10mm In  \emph{Petersen and Li} work \cite{Li95multiprocessorcache},  they present a comparative analysis of the performance achived by VM-based algorithm relative to \emph{no caching of shared data} (NO) and the \emph{MESI, invalidation-based, snoopy cache protocol} (Snoop). \\
As it is possible to observe from Figure \ref{fig:result1}, the NO, as expected, it has poor performance for most applications. It is possible to see that there can be a signicant performance gain from using VM-SC, rather than no-caching. However MUL performs best with no caching because only a little over 50% of all shared accesses hit in the cache. \par 
\parindent 10mm The best algorithm VM-based that they proposed is the RC version ( Figure \ref{fig:result1}). In fact the overhead due to coherency faults decreases in VM-RC because of the relaxed memory consistency restrictions, and accounts for most of the improvement of each application. They then compared RC with Snoop; the execution time breakdown for each application shows two reasons for the difference in performance between VM-RC and Snoop: the overhead due to the VM-based implementation of RC, the time a processor stalls on a full write-buffer, and the time a processors spends waiting for memory loads to complete. The cache miss ratio under Snoop and VM-RC is very similar for all applications, therefore the number of memory accesses due to shared and private accesses that require a bus transaction is very close. The total memory access time in VM-RC is affected mostly by memory  accesses issued by the algorithm and due to write-throughs. Relaxing the memory consistency models, and hence synchronizing the processors in the system only  when the application requires to do so, allows the VM-based RC cache coherence scheme to perform very well. 


\begin{figurehere}
 \centering
 \includegraphics[width=6cm, height=3cm]{./eps/Legenda_VM-based.PNG}
 \caption{Legend of next column diagram}
 \label{fig:legend}
\end{figurehere}

\begin{figure*}
 \centering
 \includegraphics[width=14cm, height=7cm]{./eps/result1.PNG}
 \caption{Performance of VM-based schemes}
 \label{fig:result1}
\end{figure*}

\parindent 10mm In their articles,\emph{ Kontothanassis and Scott} \cite{Kontothanassis94softwarecache} used virtual memory protection bits to enforce consistency at the granularity of pages allowing more than one processor to write a page concurrently,and used a variant of release consistency to limit coherence operations to synchronization points.\\
As in the work of \emph{Petersen and Li} \cite{Li95multiprocessorcache}, they exploit the global physical address space to move data at the granularity of cache lines: instead of copying pages, thay  map them remotely, and allow the hardware to fetch cache lines on demand. The protocol employs a distributed, non-replicated directory data structure that maintains cacheability and sharing information, similar to the coherent map data structure. A page can be in one of the following four states:
\begin{itemize}
    \item \textbf{Uncached}, no processor has a mapping to the page. This
is the initial state for all pages.
    \item \textbf{Shared}, one or more processors have read-only mappings
to the page.
   \item \textbf{Dirty}, a single processor has both read and write mappings
to the page.
   \item \textbf{Weak},  two or more processors have mappings to the page
and at least one has both read and write mappings.
\end{itemize}

They, then, compared their software protocol to the protocol devised by \emph{Petersen and Li} \cite{Li95multiprocessorcache}. They found out that their implementation was significantly better then the other and even on applications that in programs where coherence is less important, their protocols still provide reasonable performance improvements over the remaining ones, ranging from 2\% to 35\%.

\begin{figurehere}
 \centering
 \includegraphics[width=7.5cm, height=6cm]{./eps/result16proc.png}
 \caption{comparative software and hardware system performance on 16 processors}
 \label{fig:16proc}
\end{figurehere}


\begin{figurehere}
 \centering
 \includegraphics[width=7.5cm, height=6cm]{./eps/result64proc.png}
 \caption{comparative software and hardware system performance on 64 processors}
 \label{fig:64proc}
\end{figurehere}

Int the end, however, they compared their software solutions to hardware existing solutions and find out that in all cases the performance of the software protocol is within 55\% of the hardware protocol as shown in Figure \ref{fig:16proc} and in Figure \ref{fig:64proc}.\par
\parindent 10mm Thus, in the following section will be shown a couple of hardware solutions tipically found on cc-NUMA.

\subsubsection{Cache coherence in CC-NUMA}
As previously said, CC-NUMA have hardware solutions to manage coherency and there exists two different implementation depending on the architecture: BUS or an interconnection structure. According to the implemented architecture, a different solution is used to update and control caches: snooping and directory-based. It is important to choose a good protocol for cache coherence on a cache coherent system, thus write invalidate and write update have been examined to choose the best.\par
\parindent 10mm Update protocol conserves bandwidth and needs only one update  transaction to keep multiple sharers up to date but generates a lot of unnecessary traffic when a single processor writes  repeatedly to a block and no other processor accesses the block. On the other hand, invalidate does exactly the opposite and this means that there is not a best solution because both depend on the kind of application running.\par
\parindent 10mm As previously said, there are two hardware architecture: with a bus or with an interconnection structure. Nowadays it is common knowledge that using a bus architecture with a large number of nodes doesn't work very well because this solution presents some problems due to the fact that every request is broadcasted on the BUS and this tends to overfill the bandwidth. Even an interconnection structure has some problems, typically the overhead of directory indirection and message sequencing, but it is a fair price to pay compared to the benefits.\cite{moh}\par
\parindent 10mm In \emph{Majo and Gross} work, they show that in some cases, when the machine is highly loaded, the cross-chip interconnect outperforms the on-chip memory controller. Mapping computations so that all memory traffic flows through the local memory interface is bound to be suboptimal in many situations. \cite{Majo_memorysystem}

%-----------------------------------------------------------------------------
\subsection{Scheduling and memory management \\ Section}

\begin{figurehere}
 \centering
 \includegraphics[width=8cm, height=4cm]{./eps/tabella.png}
 \caption{Comparison of performance factors of static and dynamic policies}
 \label{fig:tabella}
\end{figurehere}


\section{Conclusion}
We want to discuss and recap about the results reached by our references, proposing an our personally opinion concerning the NUMA architecture issues.
Cache coherence is an important issue in multiprocessor systems and we have seen different solutions, both software and hardware. It is our opinion that nowadays the best solution is an hardware solution with an interconnection structure on most NUMA.

Software solutions, even if are not as performing as hardware ones, remain a less expensive solution good enough for NUMA with a small number of nodes and with low request performance and are, instead, really good for NUMA with many nodes and BUS architecture.

Probably some hybrid solution, like using a bus within processors on the same node and an interconnected structure to connect different nodes or even a mixture of both hardware and software solution within the same system, could lead to good results and should be investigate.
However the best thing to seek now is a way to know which resources a program will access and if this is done a lot of times or not, leading to the best choice of the protocol to be used, update or invalidate, bestowing a good speed up of performances.

% We suggest the use of JabRef for editing your bibliography file (Report.bib)
\bibliographystyle{splncs}
\bibliography{Report}

\end{multicols}
\end{document}
